# PIKA Docker Compose Configuration
#
# GPU Configuration:
# ------------------
# A GPU is required for acceptable LLM inference performance.
# Uncomment the section matching your GPU vendor below.
#
# NVIDIA: Requires NVIDIA Container Toolkit
#   https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html
#   Verify: docker run --rm --gpus all nvidia/cuda:12.6.3-base-ubuntu24.04 nvidia-smi
#
# AMD: Requires ROCm-compatible GPU (RX 6000/7000, MI series)
#   https://rocm.docs.amd.com/projects/install-on-linux/en/latest/
#   Uses ollama/ollama:rocm image instead of ollama/ollama:latest
#
# Resource Limits:
# ----------------
# Adjust these based on your hardware:
#
# - Ollama (8GB default): Larger models need more RAM
#   - 7B models: 8GB minimum
#   - 13B models: 16GB recommended
#   - 70B models: 48GB+ required
#
# - PIKA App (2GB default): Handles embeddings and ChromaDB
#   - Can reduce to 1GB for small document sets
#   - Increase to 4GB for very large indexes (10k+ documents)
#
# To disable limits (use all available resources), remove the deploy section.

services:
  ollama:
    # -- NVIDIA GPU: use the default image
    image: ollama/ollama:latest
    # -- AMD GPU: uncomment the line below and comment out the NVIDIA image above
    # image: ollama/ollama:rocm
    container_name: pika-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama
    environment:
      # Context length for generation (llama3.2 supports up to 128k)
      - OLLAMA_CONTEXT_LENGTH=4096
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    # -- AMD GPU: uncomment the lines below and comment out the NVIDIA GPU section
    # devices:
    #   - /dev/kfd
    #   - /dev/dri
    deploy:
      resources:
        limits:
          # Memory limit for Ollama - adjust based on model size
          # 8GB works for 7B-8B models, increase for larger models
          memory: 8G
        reservations:
          # Guaranteed memory - helps prevent OOM kills
          memory: 4G
          # -- NVIDIA GPU
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
          # -- AMD GPU: remove the NVIDIA devices block above; AMD uses
          #    top-level "devices" (see above) instead of deploy reservations

  pika:
    image: ghcr.io/forcesensitivesaiyan/pika:latest
    container_name: pika-app
    ports:
      - "8000:8000"
    volumes:
      - ./documents:/app/documents
      - pika_data:/app/data
    environment:
      - DEBUG=false
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_MODEL=llama3.2:3b
      - OLLAMA_TIMEOUT=300
      - DOCUMENTS_DIR=/app/documents
      - CHROMA_PERSIST_DIR=/app/data/chroma
      - AUDIT_LOG_PATH=/app/data/audit.log
    depends_on:
      ollama:
        condition: service_started
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          # Memory limit for PIKA app (embeddings + ChromaDB)
          # 2GB is sufficient for most use cases
          memory: 2G
        reservations:
          # Guaranteed memory
          memory: 512M

volumes:
  ollama_models:
    name: pika_ollama_models
  pika_data:
    name: pika_data
